{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "eceee3e9-1a11-40f6-89f7-647ffa1d8f8e",
   "metadata": {},
   "source": [
    "# Topic-Wordnet and Word Sense Disambiguation| NLP Assignment-2| Group-111"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b866d289-90b8-4951-bd5d-ca0b1b6ca3d6",
   "metadata": {},
   "source": [
    "### Group Members Name with Student ID and Contribution:\n",
    "### Student 1: AHIRE PAWAN ANIL (2023AC05618) - 100%\n",
    "### Student 2: TUSHAR SHANDILYA (2023AC05579) - 100%\n",
    "### Student 3: SAURABH JALENDRA (2023AC05912) - 100%\n",
    "### Student 4: MONICA MALIK (2023AC05875) - 100%\n",
    "### Student 5: KOLEKAR SMITA RAJESH (2023AC05092) - 100%"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d32e1c0-97e1-4ba6-a2a0-69b2fd2bc72f",
   "metadata": {},
   "source": [
    "## 1. a)DDownload the datase  b)Perform pre-processing steps  c)Normalize review column by using Stemming or Lemmatization  d)Demonstrate use of Sentiwordnet to calculate the senti_scoret"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5e31fd41-e314-49bd-8580-9610f1e49785",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a74b510-2af5-4ba6-b7fb-72d1e210b736",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "MOVIE = pd.read_csv(\"Movie.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "979359ee-7357-4712-9993-814980e74503",
   "metadata": {},
   "source": [
    "#### The dataset is loaded into a DataFrame named MOVIE. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b547845-e11f-4bcf-ba46-6fc3563cc630",
   "metadata": {},
   "source": [
    "### Justification\n",
    "The dataset contains two columns: text (movie reviews) and label (binary labels, likely 0 for negative and 1 for positive sentiment).\n",
    "\n",
    "The dataset is successfully loaded, and the DataFrame is ready for further processing.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "82524380-ac0d-42e7-98ae-186f4b6d80ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  label\n",
      "0  I grew up (b. 1965) watching and loving the Th...      0\n",
      "1  When I put this movie in my DVD player, and sa...      0\n",
      "2  Why do people who do not know what a particula...      0\n",
      "3  Even though I have great interest in Biblical ...      0\n",
      "4  Im a die hard Dads Army fan and nothing will e...      1\n",
      "5  A terrible movie as everyone has said. What ma...      0\n",
      "6  Finally watched this shocking movie last night...      1\n",
      "7  I caught this film on AZN on cable. It sounded...      0\n",
      "8  It may be the remake of 1987 Autumn's Tale aft...      1\n",
      "9  My Super Ex Girlfriend turned out to be a plea...      1\n"
     ]
    }
   ],
   "source": [
    "# Check the first few rows\n",
    "print(MOVIE.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39bb971a-00a4-4896-9e64-9016ca33c294",
   "metadata": {},
   "source": [
    "#### Displays the first 10 rows of the dataset. Each row contains a movie review and its corresponding label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19e3725a-3a2a-4554-be2f-699e09a7137d",
   "metadata": {},
   "source": [
    "### Justification\n",
    "The head() function displays the first 5 rows of the dataset.\n",
    "\n",
    "The text column contains the movie reviews, and the label column contains the sentiment labels (0 or 1).\n",
    "\n",
    "This output confirms that the dataset is structured correctly and contains the expected data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "760355db-eabd-4516-a464-4958ce98be81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 40000 entries, 0 to 39999\n",
      "Data columns (total 2 columns):\n",
      " #   Column  Non-Null Count  Dtype \n",
      "---  ------  --------------  ----- \n",
      " 0   text    40000 non-null  object\n",
      " 1   label   40000 non-null  int64 \n",
      "dtypes: int64(1), object(1)\n",
      "memory usage: 625.1+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "# Get dataset information\n",
    "print(MOVIE.info())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04fcf242-dcdb-4166-89f5-36229e5b7e70",
   "metadata": {},
   "source": [
    "#### Provides information about the dataset, including the number of entries (40,000), the columns (text and label), and their data types. It also shows that there are no missing values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a1aa66e-981f-4650-a5e8-40925c802e94",
   "metadata": {},
   "source": [
    "### Justification\n",
    "This output confirms that the dataset is clean and ready for preprocessing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ba42df72-9288-44e9-93b9-c3ebb6c5e38f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                     text         label\n",
      "count                                               40000  40000.000000\n",
      "unique                                              39723           NaN\n",
      "top     Hilarious, clean, light-hearted, and quote-wor...           NaN\n",
      "freq                                                    4           NaN\n",
      "mean                                                  NaN      0.499525\n",
      "std                                                   NaN      0.500006\n",
      "min                                                   NaN      0.000000\n",
      "25%                                                   NaN      0.000000\n",
      "50%                                                   NaN      0.000000\n",
      "75%                                                   NaN      1.000000\n",
      "max                                                   NaN      1.000000\n"
     ]
    }
   ],
   "source": [
    "# Get statistical summary\n",
    "print(MOVIE.describe(include='all'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6936048c-60e1-446d-8dcf-bb270b8c7202",
   "metadata": {},
   "source": [
    "#### Provides a statistical summary of the dataset. For the text column, it shows the count, unique values, most frequent text, and frequency of the most common text. For the label column, it shows the mean, standard deviation, min, max, and quartiles."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "518cf06b-4f3e-4c96-a60b-b4754bebdede",
   "metadata": {},
   "source": [
    "### Justification\n",
    "#### For the text column:\n",
    "\n",
    "Count: 40,000 non-null entries.\n",
    "\n",
    "Unique: 39,723 unique reviews, meaning some reviews are repeated.\n",
    "\n",
    "Top: The most frequent review is shown, along with its frequency (4  im### es).\n",
    "\n",
    "For the label column:\n",
    "\n",
    "Mean: 0.4995, indicating that the dataset is almost balanced (close to 50% positive and 50% negative reviews).\n",
    "\n",
    "Std: 0.500006, showing the standard deviation of the labels.\n",
    "\n",
    "Min/Max: Labels range from 0 to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43fcd1c0-adaf-4d46-bb44-d2b01fe239b6",
   "metadata": {},
   "source": [
    "### 1b. Preprocessing: Remove Punctuations, Numbers, Special Characters, and Stop Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9cfef4d6-a43d-4125-b814-2cced8231a7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\tushandilya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                text  \\\n",
      "0  I grew up (b. 1965) watching and loving the Th...   \n",
      "1  When I put this movie in my DVD player, and sa...   \n",
      "2  Why do people who do not know what a particula...   \n",
      "3  Even though I have great interest in Biblical ...   \n",
      "4  Im a die hard Dads Army fan and nothing will e...   \n",
      "5  A terrible movie as everyone has said. What ma...   \n",
      "6  Finally watched this shocking movie last night...   \n",
      "7  I caught this film on AZN on cable. It sounded...   \n",
      "8  It may be the remake of 1987 Autumn's Tale aft...   \n",
      "9  My Super Ex Girlfriend turned out to be a plea...   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0  grew b watching loving thunderbirds mates scho...  \n",
      "1  put movie dvd player sat coke chips expectatio...  \n",
      "2  people know particular time past like feel nee...  \n",
      "3  even though great interest biblical movies bor...  \n",
      "4  im die hard dads army fan nothing ever change ...  \n",
      "5  terrible movie everyone said made laugh cameo ...  \n",
      "6  finally watched shocking movie last night dist...  \n",
      "7  caught film azn cable sounded like would good ...  \n",
      "8  may remake autumns tale eleven years director ...  \n",
      "9  super ex girlfriend turned pleasant surprise r...  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download stopwords\n",
    "nltk.download('stopwords')\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def preprocess_text(text):\n",
    "    # Remove punctuations, numbers, and special characters\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    text = re.sub(r'\\d+', '', text)\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Remove stopwords\n",
    "    text = ' '.join([word for word in text.split() if word not in stop_words])\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Apply preprocessing to the 'text' column\n",
    "MOVIE['cleaned_text'] = MOVIE['text'].apply(preprocess_text)\n",
    "\n",
    "# Check the cleaned text\n",
    "print(MOVIE[['text', 'cleaned_text']].head(10))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52aa2b42-6ace-4d27-b2d6-97f66557ae41",
   "metadata": {},
   "source": [
    "#### The cleaned_text column is created, which contains the preprocessed text after a)Removing punctuations, numbers, and special characters. b)Removing stopwords.The output shows the original text and the cleaned text for the first 10 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c86365a6-9643-4dcb-aa6e-9993b2a4084c",
   "metadata": {},
   "source": [
    "### Justification:\n",
    "\n",
    "The preprocessing function:\n",
    "\n",
    "Removes punctuations and numbers using regex.\n",
    "\n",
    "Converts text to lowercase.\n",
    "\n",
    "Removes stopwords (common words like \"I,\" \"the,\" \"and\" that do not contribute much to sentiment).\n",
    "\n",
    "The output shows the original text and the cleaned text for the first 10 rows. The cleaned text is free of noise and ready for further analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d8c50d1-e2b6-499e-b5be-c1f3696c0cd2",
   "metadata": {},
   "source": [
    "### 1c. Normalize the review column using Stemming or Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "740f4e23-8b63-4110-8f49-c20e8b1ae1c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\tushandilya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        cleaned_text  \\\n",
      "0  grew b watching loving thunderbirds mates scho...   \n",
      "1  put movie dvd player sat coke chips expectatio...   \n",
      "2  people know particular time past like feel nee...   \n",
      "3  even though great interest biblical movies bor...   \n",
      "4  im die hard dads army fan nothing ever change ...   \n",
      "5  terrible movie everyone said made laugh cameo ...   \n",
      "6  finally watched shocking movie last night dist...   \n",
      "7  caught film azn cable sounded like would good ...   \n",
      "8  may remake autumns tale eleven years director ...   \n",
      "9  super ex girlfriend turned pleasant surprise r...   \n",
      "\n",
      "                                     lemmatized_text  \n",
      "0  grew b watching loving thunderbird mate school...  \n",
      "1  put movie dvd player sat coke chip expectation...  \n",
      "2  people know particular time past like feel nee...  \n",
      "3  even though great interest biblical movie bore...  \n",
      "4  im die hard dad army fan nothing ever change g...  \n",
      "5  terrible movie everyone said made laugh cameo ...  \n",
      "6  finally watched shocking movie last night dist...  \n",
      "7  caught film azn cable sounded like would good ...  \n",
      "8  may remake autumn tale eleven year director ma...  \n",
      "9  super ex girlfriend turned pleasant surprise r...  \n"
     ]
    }
   ],
   "source": [
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download WordNet\n",
    "nltk.download('wordnet')\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "def lemmatize_text(text):\n",
    "    return ' '.join([lemmatizer.lemmatize(word) for word in text.split()])\n",
    "\n",
    "# Apply lemmatization to the 'cleaned_text' column\n",
    "MOVIE['lemmatized_text'] = MOVIE['cleaned_text'].apply(lemmatize_text)\n",
    "\n",
    "# Check the lemmatized text\n",
    "print(MOVIE[['cleaned_text', 'lemmatized_text']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d82ab811-1c08-440d-8ac7-9d0852114226",
   "metadata": {},
   "source": [
    "#### We have used Lemmitization for the normalization process. The lemmatized_text column is created, which contains the lemmatized version of the cleaned text. The output shows the cleaned text and the lemmatized text for the first 10 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e4a018a-f9ea-428d-a10c-4e8b952b7521",
   "metadata": {},
   "source": [
    "### Justification:\n",
    "\n",
    "Lemmatization reduces words to their base or root form (e.g., \"watching\" → \"watch,\" \"movies\" → \"movie\").\n",
    "\n",
    "The output shows the cleaned text and the lemmatized text for the first 10 rows. Lemmatization ensures that different forms of the same word are treated as a single entity, improving the consistency of the text data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83fffcae-a75e-4be7-96be-e8372ee18dea",
   "metadata": {},
   "source": [
    "### 1d. Use SentiWordNet to calculate the sentiment score of each review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a967208-8182-4073-8fb7-40022d85c3ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\tushandilya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package sentiwordnet to\n",
      "[nltk_data]     C:\\Users\\tushandilya\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package sentiwordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                     lemmatized_text  senti_score\n",
      "0  grew b watching loving thunderbird mate school...        1.875\n",
      "1  put movie dvd player sat coke chip expectation...        1.750\n",
      "2  people know particular time past like feel nee...        0.000\n",
      "3  even though great interest biblical movie bore...       -0.125\n",
      "4  im die hard dad army fan nothing ever change g...        1.625\n",
      "5  terrible movie everyone said made laugh cameo ...       -1.375\n",
      "6  finally watched shocking movie last night dist...        3.875\n",
      "7  caught film azn cable sounded like would good ...        0.603\n",
      "8  may remake autumn tale eleven year director ma...        2.375\n",
      "9  super ex girlfriend turned pleasant surprise r...       10.875\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import sentiwordnet as swn\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "# Download necessary NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('sentiwordnet')\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    pos_score = 0\n",
    "    neg_score = 0\n",
    "    for token in tokens:\n",
    "        synsets = list(swn.senti_synsets(token))\n",
    "        if synsets:\n",
    "            synset = synsets[0]  # Use the first synset\n",
    "            pos_score += synset.pos_score()\n",
    "            neg_score += synset.neg_score()\n",
    "    return pos_score - neg_score  # Net sentiment score\n",
    "\n",
    "# Apply sentiment scoring to the 'lemmatized_text' column\n",
    "MOVIE['senti_score'] = MOVIE['lemmatized_text'].apply(get_sentiment_score)\n",
    "\n",
    "# Check the sentiment scores\n",
    "print(MOVIE[['lemmatized_text', 'senti_score']].head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16fbc7a4-4797-420b-97d2-71359560c60d",
   "metadata": {},
   "source": [
    "#### The senti_score column is created, which contains the sentiment score for each review. The output shows the lemmatized text and the corresponding sentiment score for the first 10 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3803b7ce-c87c-40e8-9885-d2b6a1ca83e5",
   "metadata": {},
   "source": [
    "### Justification:\n",
    "\n",
    "SentiWordNet assigns positive (pos_score) and negative (neg_score) sentiment scores to words.\n",
    "\n",
    "The net sentiment score is calculated as pos_score - neg_score.\n",
    "\n",
    "The output shows the lemmatized text and the corresponding sentiment score for the first 5 rows. Higher scores indicate more positive sentiment, while lower or negative scores indicate more negative sentiment."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f47f140-1ac0-4814-8e75-3d5c84c33b9c",
   "metadata": {},
   "source": [
    "## 2. Take some sample reviews from the dataset and demonstrate the following: (5 Marks)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d6644f-2522-4034-a7cf-b575cd14f280",
   "metadata": {},
   "source": [
    "### a. Synsets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8a1c3ab4-7543-4897-882c-380bbc8ece40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synsets for 'disappointed': [Synset('disappoint.v.01'), Synset('defeated.s.02')]\n"
     ]
    }
   ],
   "source": [
    "from nltk.corpus import wordnet\n",
    "\n",
    "# Taking a random word Example from the review sample and displaying it's Synests\n",
    "word = \"disappointed\"\n",
    "synsets = wordnet.synsets(word)\n",
    "print(f\"Synsets for '{word}': {synsets}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d84ac75e-09d8-438f-aad3-0386bb4bc38a",
   "metadata": {},
   "source": [
    "#### Synsets are sets of synonyms in WordNet that represent different senses of a word.Each synset contains information about the word's meaning, part of speech, and usage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90fd7b2e-75ce-4d28-9637-869e0f17a4a8",
   "metadata": {},
   "source": [
    "### Justification:\n",
    "\n",
    "Synsets are sets of synonyms that represent different senses of a word.\n",
    "\n",
    "For \"disappointed,\" there are two synsets:\n",
    "\n",
    "disappoint.v.01: The verb sense, meaning to fail to meet expectations.\n",
    "\n",
    "defeated.s.02: The adjective sense, meaning feeling let down or defeated.\n",
    "\n",
    "This output demonstrates how WordNet captures multiple meanings of a word."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db397da-30bf-4d3c-90dc-af164753708e",
   "metadata": {},
   "source": [
    "### b. Synonyms and Antonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "73effb0d-f685-4b5b-a0c1-79305e648467",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset: interest.n.01\n",
      "Synonyms: ['interest', 'involvement']\n",
      "Antonyms: []\n",
      "Synset: sake.n.01\n",
      "Synonyms: ['sake', 'interest']\n",
      "Antonyms: []\n",
      "Synset: interest.n.03\n",
      "Synonyms: ['interest', 'interestingness']\n",
      "Antonyms: ['uninterestingness']\n",
      "Synset: interest.n.04\n",
      "Synonyms: ['interest']\n",
      "Antonyms: []\n",
      "Synset: interest.n.05\n",
      "Synonyms: ['interest', 'stake']\n",
      "Antonyms: []\n",
      "Synset: interest.n.06\n",
      "Synonyms: ['interest', 'interest_group']\n",
      "Antonyms: []\n",
      "Synset: pastime.n.01\n",
      "Synonyms: ['pastime', 'interest', 'pursuit']\n",
      "Antonyms: []\n",
      "Synset: interest.v.01\n",
      "Synonyms: ['interest']\n",
      "Antonyms: ['bore']\n",
      "Synset: concern.v.02\n",
      "Synonyms: ['concern', 'interest', 'occupy', 'worry']\n",
      "Antonyms: []\n",
      "Synset: matter_to.v.01\n",
      "Synonyms: ['matter_to', 'interest']\n",
      "Antonyms: []\n"
     ]
    }
   ],
   "source": [
    "# Taking a random word Example from the review sample and displaying it's Synonyms and Antonyms\n",
    "word = \"interest\"\n",
    "synsets = wordnet.synsets(word)\n",
    "for synset in synsets:\n",
    "    print(f\"Synset: {synset.name()}\")\n",
    "    print(f\"Synonyms: {[lemma.name() for lemma in synset.lemmas()]}\")\n",
    "    print(f\"Antonyms: {[lemma.antonyms()[0].name() for lemma in synset.lemmas() if lemma.antonyms()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab08ba8e-4562-4679-857a-1c0fabe2ac4a",
   "metadata": {},
   "source": [
    "#### Synonyms: Lists words that have similar meanings in that context,Antonyms: Lists words with opposite meanings. Displays the synsets, synonyms, and antonyms for the word \"difficult\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a2e364f-34db-4f73-a51d-bac4bcd0c7d9",
   "metadata": {},
   "source": [
    "### Justification:\n",
    "\n",
    "For each synset of \"interest,\" the code retrieves synonyms and antonyms.\n",
    "\n",
    "For example, in interest.n.03, the synonym is \"interestingness,\" and the antonym is \"uninterestingness.\"\n",
    "\n",
    "This output demonstrates how WordNet captures relationships between words, including synonyms and antonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5b247b-9d50-44aa-9d7a-97f3ec53b9a3",
   "metadata": {},
   "source": [
    "### c. Hyponym and Hypernym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "306424c6-bd49-4f2d-834c-1f3226a82efc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Synset: people.n.01\n",
      "Hyponyms: ['age_group.n.01', 'ancients.n.01', 'baffled.n.01', 'blind.n.01', 'blood.n.05', 'brave.n.02', 'business_people.n.01', 'chosen_people.n.01', 'class.n.03', 'clientele.n.01', 'coevals.n.01', 'damned.n.01', 'dead.n.01', 'deaf.n.01', 'defeated.n.01', 'disabled.n.01', 'doomed.n.01', 'enemy.n.03', 'episcopacy.n.01', 'folk.n.01', 'free.n.01', 'homebound.n.01', 'initiate.n.03', 'living.n.02', 'lobby.n.02', 'mentally_retarded.n.01', 'migration.n.02', 'nation.n.02', 'nationality.n.01', 'network_army.n.01', 'peanut_gallery.n.01', 'peoples.n.01', 'pocket.n.07', 'poor_people.n.01', 'populace.n.01', 'population.n.01', 'rank_and_file.n.02', 'retreated.n.01', 'rich_people.n.01', 'sick.n.01', 'smart_money.n.03', 'timid.n.01', 'tradespeople.n.01', 'unconfessed.n.01', 'unemployed_people.n.01', 'uninitiate.n.01', 'womankind.n.01', 'wounded.n.01']\n",
      "Hypernyms: ['group.n.01']\n",
      "Synset: citizenry.n.01\n",
      "Hyponyms: ['achaean.n.02', 'aeolian.n.02', 'country_people.n.01', 'dorian.n.02', 'electorate.n.01', 'governed.n.01', 'ionian.n.02']\n",
      "Hypernyms: ['group.n.01']\n",
      "Synset: people.n.03\n",
      "Hyponyms: []\n",
      "Hypernyms: ['family.n.04']\n",
      "Synset: multitude.n.03\n",
      "Hyponyms: ['audience.n.02', 'following.n.01', 'laity.n.01']\n",
      "Hypernyms: ['group.n.01']\n",
      "Synset: people.v.01\n",
      "Hyponyms: []\n",
      "Hypernyms: ['populate.v.02']\n",
      "Synset: people.v.02\n",
      "Hyponyms: []\n",
      "Hypernyms: ['populate.v.01']\n"
     ]
    }
   ],
   "source": [
    "# Taking a random word Example from the movie review sample and displaying it's Hyponym and Hypernym\n",
    "word = \"people\"\n",
    "synsets = wordnet.synsets(word)\n",
    "for synset in synsets:\n",
    "    print(f\"Synset: {synset.name()}\")\n",
    "    print(f\"Hyponyms: {[lemma.name() for lemma in synset.hyponyms()]}\")\n",
    "    print(f\"Hypernyms: {[lemma.name() for lemma in synset.hypernyms()]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3dbb84ba-39f7-45fc-8d44-c0ceb9a918d1",
   "metadata": {},
   "source": [
    "#### This demonstrates the hierarchical structure of WordNet, where words are organized into broader (hypernyms) and narrower (hyponyms) categories.Displays the hyponyms (more specific terms) and hypernyms (more general terms) for the word \"people\"."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08b4b819-87ce-447b-8375-5e1b0f08a1af",
   "metadata": {},
   "source": [
    "### Justification:\n",
    "\n",
    "Hyponyms are more specific terms (e.g., \"age_group\" is a type of \"people\").\n",
    "\n",
    "Hypernyms are more general terms (e.g., \"people\" is a type of \"group\").\n",
    "\n",
    "This output demonstrates the hierarchical structure of WordNet, where words are organized into broader (hypernyms) and narrower (hyponyms) categories.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6e055a-25d4-4b05-a235-03a82f336806",
   "metadata": {},
   "source": [
    "### d. WordNet Path Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d841c14c-c454-4245-a02d-b11c2af799bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path similarity between 'actor' and 'director': 0.125\n"
     ]
    }
   ],
   "source": [
    "# Taking two random words Example from the movie review sample and displaying it's WordNet Path Similarity\n",
    "word1 = \"actor\"\n",
    "word2 = \"director\"\n",
    "synset1 = wordnet.synsets(word1)[0]\n",
    "synset2 = wordnet.synsets(word2)[0]\n",
    "similarity = synset1.path_similarity(synset2)\n",
    "print(f\"Path similarity between '{word1}' and '{word2}': {similarity}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1c9bd59-5df8-493c-85a8-64f6bb5403a3",
   "metadata": {},
   "source": [
    "#### Displays the path similarity between the words \"actor\" and \"director\".Score of 0.125 between \"actor\" and \"director\" suggests that they are somewhat related but not very close in the WordNet hierarchy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b77d516b-f14f-404b-b6cc-d2fb074514b0",
   "metadata": {},
   "source": [
    "### Justification:\n",
    "\n",
    "Path similarity measures how closely related two words are based on the shortest path between their synsets in the WordNet hierarchy.\n",
    "\n",
    "A score of 0.125 indicates that \"actor\" and \"director\" are somewhat related (both are roles in the film industry) but not very close in the hierarchy.\n",
    "\n",
    "This output demonstrates how WordNet quantifies the relatedness between words."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b8d503-b264-43f6-9bcb-1d85abbafa3a",
   "metadata": {},
   "source": [
    "### e. Word Sense Disambiguation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c866acf7-c043-475b-8a62-a9f0124e9611",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Disambiguated sense for 'production': Synset('production.n.08')\n"
     ]
    }
   ],
   "source": [
    "from nltk.wsd import lesk\n",
    "\n",
    "# Taking a random sentence Example from the movie review sample and displaying Word Sense Disambiguation of word 'Production'\n",
    "sentence = \"but the primitive production values of this film would cause any viewer to become bored\"\n",
    "word = \"production\"\n",
    "sense = lesk(sentence.split(), word)\n",
    "print(f\"Disambiguated sense for '{word}': {sense}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "234b046b-b79e-46c8-9a65-c6f9df3f5199",
   "metadata": {},
   "source": [
    "#### Word Sense Disambiguation (WSD): Determines the correct sense of a word in a given context.The Lesk algorithm is used to disambiguate the word \"production\" in the sentence.The output shows that the correct sense of \"production\" in this context is production.n.08, which refers to the process of creating something (e.g., a film).\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "888a8e81-964f-4936-a113-87a51e0fb01f",
   "metadata": {},
   "source": [
    "### Justification:\n",
    "\n",
    "The Lesk algorithm identifies the correct sense of a word in a given context.\n",
    "\n",
    "For \"production\" in the sentence, the correct sense is production.n.08, which refers to the process of creating something (e.g., a film).\n",
    "\n",
    "This output demonstrates how NLP tools can disambiguate word senses based on context."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
